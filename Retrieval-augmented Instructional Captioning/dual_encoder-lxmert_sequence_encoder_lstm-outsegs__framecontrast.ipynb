{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/lxmert/src')\n",
    "from lxrt.modeling import *\n",
    "import torch\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import easydict\n",
    "from collections import OrderedDict\n",
    "import pickle as pkl\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import transformers\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_weight = 0.05\n",
    "iou_threshold = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ious = pkl.load(open('obj_ious.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = f'lxmert_sequence_encoder_outsegs_obj_aware_{iou_threshold}_frame_seq_contrast_lxmert{loss_weight}_afterlstm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class segmentDataset(Dataset):\n",
    "    \"\"\"Segment dataset.\"\"\"\n",
    "    def __init__(self, vidlistpkl, feat_base, cook2_IVD_dir =\"/workspace/evidence_retrieval/COOK2_IVD\", mode = 'train' ):\n",
    "        with open(os.path.join(vidlistpkl), \"rb\") as f:\n",
    "            vid_pkl = pkl.load(f)\n",
    "            \n",
    "        self.vid_list = vid_pkl\n",
    "        self.feat_base = feat_base\n",
    "        with open(os.path.join(cook2_IVD_dir, f\"pkl/{mode}.pkl\"), \"rb\") as f:\n",
    "            self.vid_pkl = pkl.load(f)\n",
    "    def __len__(self):\n",
    "        return len(self.vid_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "\n",
    "        vid = self.vid_list[idx]\n",
    "        with open(os.path.join(self.feat_base, vid+'.pkl'), 'rb') as fp:\n",
    "                  item = pkl.load(fp)\n",
    "        visn_feats, encode, temporal_label, label = item\n",
    "        query = self.vid_pkl[vid][\"query\"]\n",
    "        \n",
    "        \n",
    "        \n",
    "        return {\"vid\": vid, \"visn_feats\":visn_feats, 'encode':encode, 'temporal_label':temporal_label,'label':label, 'query':query}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from statistics import mean\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import base64\n",
    "cook2_IVD_dir = \"/workspace/COOK2_IVD\"\n",
    "\n",
    "\n",
    "class segmentDataset(Dataset):\n",
    "    \"\"\"Segment dataset.\"\"\"\n",
    "    def __init__(self, vidlistpkl, feat_base, cook2_IVD_dir =\"/workspace/COOK2_IVD\", mode = 'train' ):\n",
    "        with open(os.path.join(vidlistpkl), \"rb\") as f:\n",
    "            vid_pkl = pkl.load(f)\n",
    "            \n",
    "        self.vid_list = vid_pkl\n",
    "        self.feat_base = feat_base\n",
    "        with open(os.path.join(cook2_IVD_dir, f\"pkl/{mode}.pkl\"), \"rb\") as f:\n",
    "            self.vid_pkl = pkl.load(f)\n",
    "    def __len__(self):\n",
    "        return len(self.vid_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "\n",
    "        vid = self.vid_list[idx]\n",
    "        with open(os.path.join(self.feat_base, vid+'.pkl'), 'rb') as fp:\n",
    "                  item = pkl.load(fp)\n",
    "        visn_feats,encode,label,(trans_input_ids,trans_token_type_ids,trans_attention_mask ) = item\n",
    "        query = self.vid_pkl[vid][\"query\"]\n",
    "        \n",
    "        \n",
    "        \n",
    "        return {\"vid\": vid, \"visn_feats\":visn_feats, 'encode':encode, 'label':label, 'query':query,\\\n",
    "               'trans_input_ids':trans_input_ids, 'trans_token_type_ids':trans_token_type_ids,\\\n",
    "               'trans_attention_mask':trans_attention_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_dataset = segmentDataset('newsplit_train_vid_list.pkl','feat_dump_outsegs')\n",
    "valid_dataset = segmentDataset('newsplit_valid_vid_list.pkl','feat_dump_outsegs', mode = 'vid')\n",
    "test_dataset = segmentDataset('newsplit_test_vid_list.pkl','feat_dump_outsegs', mode = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sequence_frame_encoder(torch.nn.Module):\n",
    "    def __init__(self, config, sequence_encoder = 'LSTM'):\n",
    "        super(sequence_frame_encoder, self).__init__()\n",
    "        self.frame_encoder = LXRTFeatureExtraction(config)\n",
    "        state_dict_path = os.path.join('lxmert', 'snap', 'pretrained', 'model_LXRT.pth') \n",
    "        state_dict = torch.load(state_dict_path)\n",
    "        new_state_dict = OrderedDict()\n",
    "        for key, value in state_dict.items():\n",
    "            splittedkey = key.split('.')\n",
    "            if 'bert' in splittedkey:\n",
    "                newkey  = '.'.join(splittedkey[splittedkey.index('bert')+1:])\n",
    "            else:\n",
    "                newkey  = '.'.join(splittedkey[splittedkey.index('module')+1:])\n",
    "            new_state_dict[newkey] = value\n",
    "        self.frame_encoder.load_state_dict(new_state_dict, strict=False)\n",
    "        del state_dict\n",
    "        del new_state_dict\n",
    "        if sequence_encoder == 'LSTM':\n",
    "            self.sequence_encoder = torch.nn.LSTM(768, hidden_size = 768//2, batch_first = True, bidirectional = True)\n",
    "            self.sequence_fc = torch.nn.Linear(768, 768)\n",
    "            self.contrast_fc1 = torch.nn.Linear(768,768)\n",
    "            self.contrast_fc2 = torch.nn.Linear(768,768)\n",
    "            \n",
    "        #transformer later\n",
    "        #else:\n",
    "        #    self.sequence_encoder = torch.nn.TransformerEncoderLayer(d_model = 768,nhead = 12,num_encoder_layers = 1,\n",
    "        #                                                             dim_feedforward=3072, activation == \"gelu\", batch_first = True )\n",
    "    def forward(self,visn_feats, trans_input_ids,trans_token_type_ids,trans_attention_mask ):\n",
    "        frame_feats = self.frame_encoder(trans_input_ids, \n",
    "                                        token_type_ids = trans_token_type_ids,\n",
    "                                        attention_mask = trans_attention_mask,\n",
    "                                        visual_feats = visn_feats\n",
    "                                       )\n",
    "        frame_feats = frame_feats[1]        \n",
    "        frame_contrast1 = self.contrast_fc1(frame_feats)\n",
    "        frame_contrast1 = frame_contrast1.squeeze()\n",
    "        frame_feats = frame_feats.unsqueeze(0)\n",
    "        frame_feats,(_,_) = self.sequence_encoder(frame_feats)\n",
    "        frame_contrast2 = self.contrast_fc1(frame_feats)\n",
    "        frame_contrast2 = frame_contrast2.squeeze()\n",
    "        frame_contrast = torch.cat([frame_contrast1, frame_contrast2], dim = -1)\n",
    "        #frame_contrast = self.contrast_fc(frame_feats)\n",
    "        #frame_contrast = frame_contrast.squeeze()\n",
    "        frame_feats = self.sequence_fc(frame_feats)\n",
    "        frame_feats = frame_feats.squeeze()\n",
    "        return frame_feats, frame_contrast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig('bert_config.json')\n",
    "\"\"\"image_encoder = LXRTImageModel(config)\n",
    "state_dict_path = os.path.join('lxmert', 'snap', 'pretrained', 'model_LXRT.pth') \n",
    "state_dict = torch.load(state_dict_path)\n",
    "new_state_dict = OrderedDict()\n",
    "for key, value in state_dict.items():\n",
    "    splittedkey = key.split('.')\n",
    "    if 'bert' in splittedkey:\n",
    "        newkey  = '.'.join(splittedkey[splittedkey.index('bert')+1:])\n",
    "    else:\n",
    "        newkey  = '.'.join(splittedkey[splittedkey.index('module')+1:])\n",
    "    new_state_dict[newkey] = value\n",
    "image_encoder.load_state_dict(new_state_dict, strict=False)\n",
    "image_encoder.cuda().train()\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LXRT encoder with 12 l_layers, 5 x_layers, and 0 r_layers.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig('bert_config.json')\n",
    "\"\"\"frame_encoder = LXRTFeatureExtraction(config)\n",
    "state_dict_path = os.path.join('lxmert', 'snap', 'pretrained', 'model_LXRT.pth') \n",
    "state_dict = torch.load(state_dict_path)\n",
    "new_state_dict = OrderedDict()\n",
    "for key, value in state_dict.items():\n",
    "    splittedkey = key.split('.')\n",
    "    if 'bert' in splittedkey:\n",
    "        newkey  = '.'.join(splittedkey[splittedkey.index('bert')+1:])\n",
    "    else:\n",
    "        newkey  = '.'.join(splittedkey[splittedkey.index('module')+1:])\n",
    "    new_state_dict[newkey] = value\n",
    "frame_encoder.load_state_dict(new_state_dict, strict=False)\"\"\"\n",
    "frame_encoder = sequence_frame_encoder(config)\n",
    "frame_encoder.cuda()\n",
    "#frame_encoder = nn.DataParallel(frame_encoder, device_ids=[0,1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_trans_idx(frame_idx, trans_idx):\n",
    "    return np.argmin(abs(trans_idx-frame_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "img_h, img_w = img_info['img_h'], img_info['img_w']\n",
    "boxes = boxes.copy()\n",
    "boxes[:, (0, 2)] /= img_w\n",
    "boxes[:, (1, 3)] /= img_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "lang_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "lang_encoder.cuda()\n",
    "#lang_encoder = nn.DataParallel(lang_encoder, device_ids=[0,1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxrt.optimization import BertAdam\n",
    "epoch = 100\n",
    "batch_per_epoch = len(segment_dataset)\n",
    "t_total = int(batch_per_epoch * epoch)\n",
    "warmup_ratio = 0.05\n",
    "warmup_iters = int(t_total * warmup_ratio)\n",
    "optim = BertAdam(list(frame_encoder.parameters()) + list(lang_encoder.parameters()), lr=1e-4, warmup=warmup_ratio, t_total=t_total)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lxmert_sequence_encoder_outsegs_obj_aware_0.1_frame_seq_contrast_lxmert0.05_afterlstm'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch = segment_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    vid, visn_feats, encode, label = batch['vid'], batch[\"visn_feats\"],batch['encode'],batch['label']\n",
    "    frame_feats, box_feats = visn_feats\n",
    "    foodname = batch['query']\n",
    "    trans_input_ids,trans_token_type_ids,trans_attention_mask  = batch['trans_input_ids'],batch['trans_token_type_ids'],batch['trans_attention_mask']\n",
    "    label = label.cuda()\n",
    "    visn_feats = frame_feats.cuda(), box_feats.cuda()\n",
    "    trans_input_ids = trans_input_ids.cuda()\n",
    "    trans_token_type_ids = trans_token_type_ids.cuda()\n",
    "    trans_attention_mask = trans_attention_mask.cuda()\n",
    "    encode = encode.cuda()\n",
    "    frame_feats = frame_encoder(visn_feats, trans_input_ids, \n",
    "                                        trans_token_type_ids,trans_attention_mask\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "floss_fn = torch.nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, pos_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cos = nn.CosineSimilarity(dim=0, eps=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "contrast_feat = frame_feats[1]\n",
    "frame_contrast_output = torch.matmul(contrast_feat, contrast_feat.transpose(1,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "frame_contrast_output = m(frame_contrast_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "contrast_label = []\n",
    "for i in range(len(label)):\n",
    "    c_label = []\n",
    "    for j in range(len(label)):\n",
    "        c_label.append(int(label[i]==label[j]) if label[i]>-0.5 else 0)\n",
    "    contrast_label.append(c_label)\n",
    "contrast_label = torch.tensor(contrast_label).float().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss_output = loss(frame_contrast_output, contrast_label)\n",
    "loss_output = loss_output*mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.mean(loss_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mask = (label>-0.5).float()\n",
    "mask = torch.matmul(mask.unsqueeze(-1), mask.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_forward(batch, frame_encoder, text_encoder, compute_loss = True):\n",
    "    vid, visn_feats, encode, label = batch['vid'], batch[\"visn_feats\"],batch['encode'],batch['label']\n",
    "    frame_feats, box_feats = visn_feats\n",
    "    foodname = batch['query']\n",
    "    trans_input_ids,trans_token_type_ids,trans_attention_mask  = batch['trans_input_ids'],batch['trans_token_type_ids'],batch['trans_attention_mask']\n",
    "    label = label.cuda()\n",
    "    visn_feats = frame_feats.cuda(), box_feats.cuda()\n",
    "    trans_input_ids = trans_input_ids.cuda()\n",
    "    trans_token_type_ids = trans_token_type_ids.cuda()\n",
    "    trans_attention_mask = trans_attention_mask.cuda()\n",
    "    encode = encode.cuda()\n",
    "    frame_feats,contrast_feat = frame_encoder(visn_feats, trans_input_ids, \n",
    "                                        trans_token_type_ids,trans_attention_mask\n",
    "                                       )\n",
    "    pooled_output = frame_feats\n",
    "    output = lang_encoder(encode)\n",
    "    sequence_output, lang_pooled_output = output[0], output[1]\n",
    "    dotoutput = torch.matmul(pooled_output, lang_pooled_output.transpose(1,0))\n",
    "    if compute_loss:\n",
    "        loss = loss_fn(dotoutput, label)\n",
    "        frame_contrast_output = torch.matmul(contrast_feat, contrast_feat.transpose(1,0))\n",
    "        #frame_contrast_output = sigmoid(frame_contrast_output)\n",
    "        contrast_label = []\n",
    "        for i in range(len(label)):\n",
    "            c_label = []\n",
    "            for j in range(len(label)):\n",
    "                c_label.append(int(label[i]==label[j]) if label[i]>-0.5 else 0)\n",
    "            contrast_label.append(c_label)\n",
    "        contrast_label = torch.tensor(contrast_label).float().cuda()\n",
    "        mask = (label>-0.5).float()\n",
    "        mask = torch.matmul(mask.unsqueeze(-1), mask.unsqueeze(0))\n",
    "        fcontrast_loss = floss_fn(frame_contrast_output, contrast_label)\n",
    "        fcontrast_loss = torch.mean(fcontrast_loss*mask) \n",
    "        return dotoutput, loss+loss_weight*fcontrast_loss\n",
    "    else:\n",
    "        return dotoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/workspace/lxmert/src/lxrt/optimization.py:142: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(2.1175, device='cuda:0')\n",
      "1 0.20694159842593385\n",
      "3 0.514609537812975\n",
      "5 0.7585385703036757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [10:30<17:19:58, 630.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.9259, device='cuda:0')\n",
      "1 0.20466441984772848\n",
      "3 0.5143639917360101\n",
      "5 0.7520634952620809\n",
      "1 tensor(2.0310, device='cuda:0')\n",
      "1 0.2082063267045154\n",
      "3 0.5195893505458673\n",
      "5 0.7579717974406486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [20:34<16:56:42, 622.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor(0.8917, device='cuda:0')\n",
      "1 0.20628507306084912\n",
      "3 0.5201230840621814\n",
      "5 0.7522193625963607\n",
      "2 tensor(1.9600, device='cuda:0')\n",
      "1 0.24049195687746133\n",
      "3 0.5653026288254843\n",
      "5 0.7869000431715297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [30:28<16:32:42, 614.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tensor(0.8640, device='cuda:0')\n",
      "1 0.23605196350657975\n",
      "3 0.5629100720094304\n",
      "5 0.7837535120715301\n",
      "3 tensor(1.9493, device='cuda:0')\n",
      "1 0.25172556898247894\n",
      "3 0.5817355074114319\n",
      "5 0.7940252790893059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [40:28<16:15:26, 609.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 tensor(0.8564, device='cuda:0')\n",
      "1 0.2500360517125342\n",
      "3 0.5785440874917205\n",
      "5 0.7901529061668044\n",
      "4 tensor(2.0001, device='cuda:0')\n",
      "1 0.2392252978174203\n",
      "3 0.5774464582257667\n",
      "5 0.7919402400537794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [50:30<16:01:53, 607.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 tensor(0.8710, device='cuda:0')\n",
      "1 0.23645347539956582\n",
      "3 0.5742984795764003\n",
      "5 0.7859390742811536\n",
      "5 tensor(2.0136, device='cuda:0')\n",
      "1 0.24360791910957233\n",
      "3 0.5775358319397665\n",
      "5 0.7871938065234442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [1:00:33<15:49:31, 606.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 tensor(0.8700, device='cuda:0')\n",
      "1 0.2453712820394766\n",
      "3 0.5763374465362523\n",
      "5 0.7843978904760028\n",
      "6 tensor(1.9450, device='cuda:0')\n",
      "1 0.26698829357129983\n",
      "3 0.6129203013991367\n",
      "5 0.8093373505408915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [1:10:36<15:38:01, 605.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 tensor(0.8437, device='cuda:0')\n",
      "1 0.2684415220214363\n",
      "3 0.6110146353400537\n",
      "5 0.8059063012627548\n",
      "7 tensor(1.9362, device='cuda:0')\n",
      "1 0.27930707399633226\n",
      "3 0.6116129174038388\n",
      "5 0.8131591398021185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [1:20:46<15:29:55, 606.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 tensor(0.8336, device='cuda:0')\n",
      "1 0.28367420346309274\n",
      "3 0.6094697507487229\n",
      "5 0.8120020861471179\n",
      "8 tensor(1.6855, device='cuda:0')\n",
      "1 0.37033517926729964\n",
      "3 0.715708783683573\n",
      "5 0.8831355614005585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [1:31:01<15:23:45, 609.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 tensor(0.7464, device='cuda:0')\n",
      "1 0.36690290396788466\n",
      "3 0.7114904041172907\n",
      "5 0.8804583163360896\n",
      "9 tensor(1.6483, device='cuda:0')\n",
      "1 0.3880374530324281\n",
      "3 0.7241545906651615\n",
      "5 0.8920258154659487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [1:41:05<15:11:37, 607.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 tensor(0.7397, device='cuda:0')\n",
      "1 0.38177169053444454\n",
      "3 0.7192169625915692\n",
      "5 0.8869535273284179\n",
      "10 tensor(1.6291, device='cuda:0')\n",
      "1 0.3947318437697512\n",
      "3 0.7382552775626723\n",
      "5 0.8990326945909276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [1:51:07<14:58:39, 605.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 tensor(0.7305, device='cuda:0')\n",
      "1 0.39155876698094794\n",
      "3 0.7318654797958741\n",
      "5 0.8919560603336075\n",
      "11 tensor(1.6445, device='cuda:0')\n",
      "1 0.41041946562441284\n",
      "3 0.742580761562188\n",
      "5 0.9000340248827639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [2:01:19<14:51:22, 607.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 tensor(0.7311, device='cuda:0')\n",
      "1 0.41119422026053415\n",
      "3 0.7404683057447594\n",
      "5 0.894606624580435\n",
      "12 tensor(1.5717, device='cuda:0')\n",
      "1 0.4348045067286645\n",
      "3 0.7723800793460442\n",
      "5 0.9130223976574717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [2:11:18<14:37:33, 605.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 tensor(0.6982, device='cuda:0')\n",
      "1 0.43658313639161245\n",
      "3 0.7684763212593345\n",
      "5 0.908731312222036\n",
      "13 tensor(1.5616, device='cuda:0')\n",
      "1 0.4513161452034621\n",
      "3 0.7836999356419204\n",
      "5 0.9170170800862744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [2:21:24<14:27:51, 605.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 tensor(0.6905, device='cuda:0')\n",
      "1 0.4514216354463979\n",
      "3 0.7794357963485105\n",
      "5 0.9126181796312296\n",
      "14 tensor(1.5547, device='cuda:0')\n",
      "1 0.45782564914729706\n",
      "3 0.7912451870483646\n",
      "5 0.9229265391590019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [2:31:36<14:20:20, 607.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 tensor(0.6936, device='cuda:0')\n",
      "1 0.4571098230363188\n",
      "3 0.7870257105976051\n",
      "5 0.9194229683167122\n",
      "15 tensor(1.6173, device='cuda:0')\n",
      "1 0.4546639485283191\n",
      "3 0.7852608622378315\n",
      "5 0.9224860036490339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [2:41:31<14:04:57, 603.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 tensor(0.7005, device='cuda:0')\n",
      "1 0.4561126095505195\n",
      "3 0.7812528150762793\n",
      "5 0.9187255739927089\n",
      "16 tensor(1.6253, device='cuda:0')\n",
      "1 0.4469118255111159\n",
      "3 0.7877770940381738\n",
      "5 0.9202246586782719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [2:51:44<13:59:06, 606.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 tensor(0.7227, device='cuda:0')\n",
      "1 0.4472266806327615\n",
      "3 0.7844189809414353\n",
      "5 0.916230430742059\n",
      "17 tensor(1.5630, device='cuda:0')\n",
      "1 0.4688647918306575\n",
      "3 0.8002058303881738\n",
      "5 0.9252667196299013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [3:01:45<13:46:38, 604.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 tensor(0.6810, device='cuda:0')\n",
      "1 0.4694586024929155\n",
      "3 0.7940984547730489\n",
      "5 0.9217548454400796\n",
      "18 tensor(1.5328, device='cuda:0')\n",
      "1 0.4795174476588643\n",
      "3 0.807953163062\n",
      "5 0.9327709414446901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [3:11:51<13:36:54, 605.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 tensor(0.6709, device='cuda:0')\n",
      "1 0.4793281861107256\n",
      "3 0.8021809282153244\n",
      "5 0.9282379865622766\n",
      "19 tensor(1.5720, device='cuda:0')\n",
      "1 0.4776284483768212\n",
      "3 0.8067916642108975\n",
      "5 0.9299556356783012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [3:21:44<13:21:56, 601.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 tensor(0.6992, device='cuda:0')\n",
      "1 0.48003553160198104\n",
      "3 0.7993580718599657\n",
      "5 0.9252823136269627\n",
      "20 tensor(1.6072, device='cuda:0')\n",
      "1 0.48207122437413236\n",
      "3 0.810305288393491\n",
      "5 0.9328633250638761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [3:31:40<13:09:54, 599.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 tensor(0.7041, device='cuda:0')\n",
      "1 0.48066306697085626\n",
      "3 0.8025088093382454\n",
      "5 0.9279415416396665\n",
      "21 tensor(1.5229, device='cuda:0')\n",
      "1 0.4916953919787311\n",
      "3 0.8191676850235609\n",
      "5 0.9381041673365218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [3:41:38<12:58:54, 599.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 tensor(0.6730, device='cuda:0')\n",
      "1 0.4892287248474594\n",
      "3 0.8121790891842565\n",
      "5 0.9330479706988388\n",
      "22 tensor(1.7072, device='cuda:0')\n",
      "1 0.4677569341634666\n",
      "3 0.7972406616303027\n",
      "5 0.9267872765285696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [3:51:43<12:51:29, 601.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 tensor(0.7609, device='cuda:0')\n",
      "1 0.46836050951662034\n",
      "3 0.7899393353863113\n",
      "5 0.9206814657255875\n",
      "23 tensor(1.6559, device='cuda:0')\n",
      "1 0.4941123712349781\n",
      "3 0.8144331416434322\n",
      "5 0.9352885548108179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [4:01:53<12:44:41, 603.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 tensor(0.7233, device='cuda:0')\n",
      "1 0.4926870266361109\n",
      "3 0.8075658024069864\n",
      "5 0.9300686861134665\n",
      "24 tensor(1.7270, device='cuda:0')\n",
      "1 0.4831611498515635\n",
      "3 0.8070343639438814\n",
      "5 0.9308445092785222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [4:11:47<12:30:53, 600.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 tensor(0.7455, device='cuda:0')\n",
      "1 0.4838860377010345\n",
      "3 0.8009028669184343\n",
      "5 0.9266952205965676\n",
      "25 tensor(1.9279, device='cuda:0')\n",
      "1 0.4750010130299782\n",
      "3 0.8028778862487261\n",
      "5 0.9229944167373042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [4:21:45<12:19:48, 599.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 tensor(0.8351, device='cuda:0')\n",
      "1 0.47614224361580654\n",
      "3 0.7965488422696639\n",
      "5 0.9187126956071966\n",
      "26 tensor(1.6440, device='cuda:0')\n",
      "1 0.4994630408946878\n",
      "3 0.8268740651473839\n",
      "5 0.940319007813096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [4:31:35<12:06:12, 596.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 tensor(0.7206, device='cuda:0')\n",
      "1 0.5005517980218981\n",
      "3 0.8201927920093531\n",
      "5 0.9361761667162168\n",
      "27 tensor(1.6698, device='cuda:0')\n",
      "1 0.49913743212323514\n",
      "3 0.8272823152688235\n",
      "5 0.9400829820957264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [4:41:35<11:57:21, 597.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 tensor(0.7227, device='cuda:0')\n",
      "1 0.5018106601681355\n",
      "3 0.8226987475437129\n",
      "5 0.9370479808592923\n",
      "28 tensor(1.6414, device='cuda:0')\n",
      "1 0.5149898507283281\n",
      "3 0.8319167180458723\n",
      "5 0.9421595970571562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [4:51:41<11:50:19, 600.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 tensor(0.7098, device='cuda:0')\n",
      "1 0.5168239433095241\n",
      "3 0.8268545528859138\n",
      "5 0.9395612966708791\n",
      "29 tensor(1.6443, device='cuda:0')\n",
      "1 0.5090484008255254\n",
      "3 0.8334668962553585\n",
      "5 0.9451820854279456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [5:01:48<11:42:38, 602.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 tensor(0.7291, device='cuda:0')\n",
      "1 0.5129772370749908\n",
      "3 0.8265357740054043\n",
      "5 0.9411775906247424\n",
      "30 tensor(1.6809, device='cuda:0')\n",
      "1 0.5098016984338395\n",
      "3 0.8333297645503037\n",
      "5 0.9413082691240096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [5:11:42<11:29:49, 599.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 tensor(0.7287, device='cuda:0')\n",
      "1 0.5132070313711158\n",
      "3 0.8267624824210953\n",
      "5 0.938917644779468\n",
      "31 tensor(1.7404, device='cuda:0')\n",
      "1 0.50666319554748\n",
      "3 0.830198550303667\n",
      "5 0.9408007285295767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [5:21:39<11:19:01, 599.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 tensor(0.7463, device='cuda:0')\n",
      "1 0.5116055472636061\n",
      "3 0.825679247990975\n",
      "5 0.9391601574391978\n",
      "32 tensor(1.7833, device='cuda:0')\n",
      "1 0.5073745761407915\n",
      "3 0.8315087603854119\n",
      "5 0.9418586798992118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [5:31:39<11:09:24, 599.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 tensor(0.7827, device='cuda:0')\n",
      "1 0.5099672655132507\n",
      "3 0.8251609796653916\n",
      "5 0.9392770745344253\n",
      "33 tensor(1.6586, device='cuda:0')\n",
      "1 0.5248945031701341\n",
      "3 0.8399884697798732\n",
      "5 0.9486623004386394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [5:41:36<10:58:35, 598.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 tensor(0.7219, device='cuda:0')\n",
      "1 0.5253663607914023\n",
      "3 0.8335281802888547\n",
      "5 0.9447273958857229\n",
      "34 tensor(1.7258, device='cuda:0')\n",
      "1 0.5174912088037231\n",
      "3 0.8361257609411965\n",
      "5 0.9467693970739591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [5:51:46<10:52:16, 602.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 tensor(0.7656, device='cuda:0')\n",
      "1 0.5179363591011608\n",
      "3 0.8291176398545941\n",
      "5 0.9418614034381647\n",
      "35 tensor(1.7453, device='cuda:0')\n",
      "1 0.514821865881076\n",
      "3 0.8372959981779085\n",
      "5 0.9473461362994692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [6:01:53<10:43:31, 603.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 tensor(0.7717, device='cuda:0')\n",
      "1 0.5184162544037434\n",
      "3 0.8305280242960345\n",
      "5 0.9435837561809712\n",
      "36 tensor(1.7640, device='cuda:0')\n",
      "1 0.5186504863553711\n",
      "3 0.8355525663090441\n",
      "5 0.942779878177891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [6:11:54<10:32:46, 602.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 tensor(0.7842, device='cuda:0')\n",
      "1 0.5217409426524784\n",
      "3 0.8297374582512649\n",
      "5 0.939821455980639\n",
      "37 tensor(1.7710, device='cuda:0')\n",
      "1 0.5242256111964608\n",
      "3 0.8398491929191062\n",
      "5 0.9472677470199474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [6:22:00<10:23:49, 603.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 tensor(0.7665, device='cuda:0')\n",
      "1 0.5271210724114146\n",
      "3 0.8332277882557247\n",
      "5 0.9434132150877883\n",
      "38 tensor(1.8377, device='cuda:0')\n",
      "1 0.5143324244013063\n",
      "3 0.8342647149279151\n",
      "5 0.9449902677330079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [6:32:18<10:18:06, 607.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 tensor(0.7974, device='cuda:0')\n",
      "1 0.5173515352107675\n",
      "3 0.8283629158439167\n",
      "5 0.9416170494521768\n",
      "39 tensor(1.8571, device='cuda:0')\n",
      "1 0.5119872167539429\n",
      "3 0.8375013336457101\n",
      "5 0.9485847144307058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [6:42:14<10:04:24, 604.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 tensor(0.8180, device='cuda:0')\n",
      "1 0.5164608455002676\n",
      "3 0.8318993472157412\n",
      "5 0.9440732146222032\n",
      "40 tensor(1.8737, device='cuda:0')\n",
      "1 0.5150544366814412\n",
      "3 0.8344156950978107\n",
      "5 0.9463876755798066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [6:52:21<9:55:17, 605.37s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 tensor(0.8204, device='cuda:0')\n",
      "1 0.5170886217383608\n",
      "3 0.8282711725000946\n",
      "5 0.9424981242269277\n",
      "41 tensor(1.8485, device='cuda:0')\n",
      "1 0.5185782154022524\n",
      "3 0.8387359838483766\n",
      "5 0.9484746597087497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [7:02:35<9:47:25, 607.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 tensor(0.8007, device='cuda:0')\n",
      "1 0.5242627476620346\n",
      "3 0.8341470283064829\n",
      "5 0.9453248935274743\n",
      "42 tensor(1.8487, device='cuda:0')\n",
      "1 0.5234004954493674\n",
      "3 0.8394322803552519\n",
      "5 0.9492250372086849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [7:12:33<9:34:38, 604.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 tensor(0.8163, device='cuda:0')\n",
      "1 0.525970342673874\n",
      "3 0.8352606956136852\n",
      "5 0.9460584974261388\n",
      "43 tensor(1.8798, device='cuda:0')\n",
      "1 0.529223901704969\n",
      "3 0.8406760063357374\n",
      "5 0.9480864648275625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [7:22:40<9:25:09, 605.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 tensor(0.8200, device='cuda:0')\n",
      "1 0.5304447941976499\n",
      "3 0.8357151466670952\n",
      "5 0.9453392187001336\n",
      "44 tensor(1.8518, device='cuda:0')\n",
      "1 0.5300031613640456\n",
      "3 0.8431077632723167\n",
      "5 0.9481589931940294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [7:32:43<9:14:24, 604.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 tensor(0.8014, device='cuda:0')\n",
      "1 0.5312423729958244\n",
      "3 0.8384776425669004\n",
      "5 0.9459278948616201\n",
      "45 tensor(1.8366, device='cuda:0')\n",
      "1 0.5339484918849869\n",
      "3 0.8434565062303951\n",
      "5 0.9496388335344792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [7:42:48<9:04:14, 604.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 tensor(0.7960, device='cuda:0')\n",
      "1 0.5342227392656573\n",
      "3 0.8397504357592477\n",
      "5 0.9462411226714\n",
      "46 tensor(1.9254, device='cuda:0')\n",
      "1 0.5284766630127865\n",
      "3 0.840237415137712\n",
      "5 0.9476332188886081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [7:52:48<8:53:09, 603.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 tensor(0.8424, device='cuda:0')\n",
      "1 0.5311321335911295\n",
      "3 0.8360613823239156\n",
      "5 0.9450967723218833\n",
      "47 tensor(1.9434, device='cuda:0')\n",
      "1 0.5299701327160447\n",
      "3 0.8434956603213885\n",
      "5 0.9483855815381375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [8:02:50<8:42:28, 602.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 tensor(0.8424, device='cuda:0')\n",
      "1 0.5315170727743089\n",
      "3 0.8380245247807866\n",
      "5 0.9442138661470203\n",
      "48 tensor(1.8920, device='cuda:0')\n",
      "1 0.5294087180658723\n",
      "3 0.8432264612176045\n",
      "5 0.9503057121663409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [8:12:57<8:33:33, 604.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 tensor(0.8256, device='cuda:0')\n",
      "1 0.5306687843061404\n",
      "3 0.838222962435314\n",
      "5 0.9465431544565192\n",
      "49 tensor(1.9203, device='cuda:0')\n",
      "1 0.5305926211618984\n",
      "3 0.8442897143039155\n",
      "5 0.9518573172305281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [8:22:59<8:22:56, 603.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 tensor(0.8349, device='cuda:0')\n",
      "1 0.5313223138702936\n",
      "3 0.8391588333889706\n",
      "5 0.9475222311519748\n",
      "50 tensor(2.0713, device='cuda:0')\n",
      "1 0.5184296867203232\n",
      "3 0.8379576781304419\n",
      "5 0.9455935839595632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [8:33:05<8:13:27, 604.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 tensor(0.8854, device='cuda:0')\n",
      "1 0.5202378863851477\n",
      "3 0.8322878510594008\n",
      "5 0.9425286272303937\n",
      "51 tensor(2.0225, device='cuda:0')\n",
      "1 0.5295277988738148\n",
      "3 0.8432833867090076\n",
      "5 0.9471131344835089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [8:43:19<8:05:50, 607.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 tensor(0.8636, device='cuda:0')\n",
      "1 0.5314266405845289\n",
      "3 0.8382727419292532\n",
      "5 0.9446242759074515\n",
      "52 tensor(2.0301, device='cuda:0')\n",
      "1 0.5294440357005815\n",
      "3 0.843202080011352\n",
      "5 0.9487758223044775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [8:53:03<7:50:11, 600.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 tensor(0.8910, device='cuda:0')\n",
      "1 0.5280331966228916\n",
      "3 0.8375634885790542\n",
      "5 0.944744547327722\n",
      "53 tensor(2.0407, device='cuda:0')\n",
      "1 0.5246711561746183\n",
      "3 0.8408325870557604\n",
      "5 0.9487933468201332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [9:03:18<7:43:36, 604.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 tensor(0.8829, device='cuda:0')\n",
      "1 0.5256805894748802\n",
      "3 0.8347800071758258\n",
      "5 0.946192527411218\n",
      "54 tensor(2.0852, device='cuda:0')\n",
      "1 0.5254580093937588\n",
      "3 0.8432794404436774\n",
      "5 0.9491016230423773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [9:13:08<7:30:04, 600.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 tensor(0.9088, device='cuda:0')\n",
      "1 0.5269260366787479\n",
      "3 0.8380488601707237\n",
      "5 0.9465821971947554\n",
      "55 tensor(2.0489, device='cuda:0')\n",
      "1 0.5325787066298873\n",
      "3 0.8457702170138993\n",
      "5 0.9488744296390417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56/100 [9:22:56<7:17:36, 596.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 tensor(0.8860, device='cuda:0')\n",
      "1 0.5333019106417952\n",
      "3 0.8410960416525068\n",
      "5 0.9464677416713404\n",
      "56 tensor(2.0523, device='cuda:0')\n",
      "1 0.5334411228200089\n",
      "3 0.8454232749523205\n",
      "5 0.9484629099249228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57/100 [9:33:00<7:09:07, 598.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 tensor(0.8646, device='cuda:0')\n",
      "1 0.5362990994943312\n",
      "3 0.8404585592023218\n",
      "5 0.9463693244116533\n",
      "57 tensor(2.0554, device='cuda:0')\n",
      "1 0.5334829653625734\n",
      "3 0.844942472618665\n",
      "5 0.9488474149103424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [9:42:59<6:59:18, 599.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 tensor(0.9173, device='cuda:0')\n",
      "1 0.5332312789110875\n",
      "3 0.8394721678778412\n",
      "5 0.9452713918326346\n",
      "58 tensor(2.0403, device='cuda:0')\n",
      "1 0.5381188873007637\n",
      "3 0.8477569863240243\n",
      "5 0.9516443420717365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59/100 [9:53:03<6:50:13, 600.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 tensor(0.8837, device='cuda:0')\n",
      "1 0.54005965376917\n",
      "3 0.8430789644980811\n",
      "5 0.9483804628312116\n",
      "59 tensor(2.1183, device='cuda:0')\n",
      "1 0.5280808577056018\n",
      "3 0.8440910563691875\n",
      "5 0.9497370989268578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [10:02:50<6:37:35, 596.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 tensor(0.9154, device='cuda:0')\n",
      "1 0.5313354852706015\n",
      "3 0.8378432009684305\n",
      "5 0.9467502882335255\n",
      "60 tensor(2.1038, device='cuda:0')\n",
      "1 0.5386175086865995\n",
      "3 0.8465477859644579\n",
      "5 0.9522938520073302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [10:12:43<6:26:53, 595.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 tensor(0.9046, device='cuda:0')\n",
      "1 0.5396191760211065\n",
      "3 0.8406092579608181\n",
      "5 0.9483493411557196\n",
      "61 tensor(2.1308, device='cuda:0')\n",
      "1 0.5283832319995154\n",
      "3 0.8442198085544194\n",
      "5 0.950637444611991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [10:22:36<6:16:41, 594.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 tensor(0.9289, device='cuda:0')\n",
      "1 0.5295784166948768\n",
      "3 0.8382034638502305\n",
      "5 0.9471638600638884\n",
      "62 tensor(2.1815, device='cuda:0')\n",
      "1 0.534794803162816\n",
      "3 0.8400493043795327\n",
      "5 0.9472611464296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63/100 [10:32:18<6:04:16, 590.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 tensor(0.9092, device='cuda:0')\n",
      "1 0.5351633514442642\n",
      "3 0.8366712683223989\n",
      "5 0.9450996691249072\n",
      "63 tensor(2.1597, device='cuda:0')\n",
      "1 0.5326380413440998\n",
      "3 0.84573524273191\n",
      "5 0.949751829563204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64/100 [10:42:14<5:55:27, 592.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 tensor(0.9481, device='cuda:0')\n",
      "1 0.533369565826474\n",
      "3 0.8395625329020169\n",
      "5 0.9469811496175012\n",
      "64 tensor(2.1566, device='cuda:0')\n",
      "1 0.5335790402294159\n",
      "3 0.8470515575033215\n",
      "5 0.9522832630206759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65/100 [10:52:01<5:44:35, 590.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 tensor(0.9254, device='cuda:0')\n",
      "1 0.5348943610908031\n",
      "3 0.8414505412914781\n",
      "5 0.9493624391919039\n",
      "65 tensor(2.2353, device='cuda:0')\n",
      "1 0.5306745739423153\n",
      "3 0.8406673352746057\n",
      "5 0.9493801928360733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66/100 [11:02:12<5:38:10, 596.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 tensor(0.9596, device='cuda:0')\n",
      "1 0.5309052807706376\n",
      "3 0.8358645566319819\n",
      "5 0.9469555741981508\n",
      "66 tensor(2.2385, device='cuda:0')\n",
      "1 0.5287361767959496\n",
      "3 0.8437308418283601\n",
      "5 0.9494721145619197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67/100 [11:12:02<5:27:11, 594.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 tensor(0.9793, device='cuda:0')\n",
      "1 0.5314869593910027\n",
      "3 0.8383023886672372\n",
      "5 0.9459844228835325\n",
      "67 tensor(2.2953, device='cuda:0')\n",
      "1 0.5316967347230782\n",
      "3 0.8415898690056485\n",
      "5 0.9489302785403683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 68/100 [11:22:03<5:18:15, 596.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 tensor(0.9917, device='cuda:0')\n",
      "1 0.5320516431191749\n",
      "3 0.8351166423069817\n",
      "5 0.945458616428121\n",
      "68 tensor(2.2981, device='cuda:0')\n",
      "1 0.5272665011636609\n",
      "3 0.8447326587425752\n",
      "5 0.9485638121800489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 69/100 [11:32:11<5:10:01, 600.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 tensor(1.0022, device='cuda:0')\n",
      "1 0.52670347476714\n",
      "3 0.8387335420485242\n",
      "5 0.9450415247762072\n",
      "69 tensor(2.3015, device='cuda:0')\n",
      "1 0.5335270674728636\n",
      "3 0.8448126638918065\n",
      "5 0.95074298669603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [11:42:16<5:00:49, 601.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 tensor(0.9962, device='cuda:0')\n",
      "1 0.5343532110485448\n",
      "3 0.8386888417420133\n",
      "5 0.9481185323466577\n",
      "70 tensor(2.2714, device='cuda:0')\n",
      "1 0.5338718610798409\n",
      "3 0.8465981274387221\n",
      "5 0.9496927413562484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [11:52:12<4:49:59, 599.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 tensor(0.9707, device='cuda:0')\n",
      "1 0.5362125186991398\n",
      "3 0.8402452751458667\n",
      "5 0.9474055679134439\n",
      "71 tensor(2.3486, device='cuda:0')\n",
      "1 0.5271615010935291\n",
      "3 0.8410014321323339\n",
      "5 0.9474540631432513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [12:02:06<4:39:03, 598.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 tensor(1.0234, device='cuda:0')\n",
      "1 0.5290113084780957\n",
      "3 0.8333760952383943\n",
      "5 0.9437608006736259\n",
      "72 tensor(2.3428, device='cuda:0')\n",
      "1 0.533531572705756\n",
      "3 0.8449308640717075\n",
      "5 0.9506918631954162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [12:12:08<4:29:38, 599.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 tensor(0.9946, device='cuda:0')\n",
      "1 0.5356363333066613\n",
      "3 0.8383759344056922\n",
      "5 0.9472540698276329\n",
      "73 tensor(2.3751, device='cuda:0')\n",
      "1 0.5315388187734198\n",
      "3 0.8445304974739998\n",
      "5 0.949785324186827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74/100 [12:22:01<4:18:54, 597.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 tensor(1.0325, device='cuda:0')\n",
      "1 0.5338010762788435\n",
      "3 0.8382127029158606\n",
      "5 0.9466413865990181\n",
      "74 tensor(2.2682, device='cuda:0')\n",
      "1 0.5438275893871111\n",
      "3 0.8484187622462889\n",
      "5 0.951223390658105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75/100 [12:31:53<4:08:14, 595.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 tensor(0.9610, device='cuda:0')\n",
      "1 0.546606305778391\n",
      "3 0.8441268180157449\n",
      "5 0.9489243926048698\n",
      "75 tensor(2.2850, device='cuda:0')\n",
      "1 0.5422712503253888\n",
      "3 0.8473077655728136\n",
      "5 0.95168948561866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 76/100 [12:42:03<4:00:00, 600.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 tensor(0.9699, device='cuda:0')\n",
      "1 0.5445976340900538\n",
      "3 0.8425377237436831\n",
      "5 0.9489852995897875\n",
      "76 tensor(2.3412, device='cuda:0')\n",
      "1 0.5344925398288233\n",
      "3 0.8450299510550848\n",
      "5 0.9506251182586539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 77/100 [12:52:05<3:50:13, 600.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 tensor(1.0007, device='cuda:0')\n",
      "1 0.5369880584844737\n",
      "3 0.8400821074455977\n",
      "5 0.9474008289256424\n",
      "77 tensor(2.4211, device='cuda:0')\n",
      "1 0.5279363457241494\n",
      "3 0.8418639803033443\n",
      "5 0.9478316741208521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 78/100 [13:02:00<3:39:36, 598.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 tensor(1.0471, device='cuda:0')\n",
      "1 0.5302795847757301\n",
      "3 0.836386067095959\n",
      "5 0.9447929595790674\n",
      "78 tensor(2.3023, device='cuda:0')\n",
      "1 0.5403743239938174\n",
      "3 0.84867044470419\n",
      "5 0.9508184266742137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 79/100 [13:11:56<3:29:21, 598.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 tensor(0.9777, device='cuda:0')\n",
      "1 0.5431753797878158\n",
      "3 0.8444293489219091\n",
      "5 0.9480752228450452\n",
      "79 tensor(2.3701, device='cuda:0')\n",
      "1 0.537283554234919\n",
      "3 0.8464845574636387\n",
      "5 0.9498011216823008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [13:22:03<3:20:15, 600.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 tensor(1.0138, device='cuda:0')\n",
      "1 0.5374347558489655\n",
      "3 0.8402678479752852\n",
      "5 0.9477683571704354\n",
      "80 tensor(2.3586, device='cuda:0')\n",
      "1 0.5357566536655737\n",
      "3 0.8483824654239303\n",
      "5 0.9504658084070141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [13:31:49<3:08:49, 596.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 tensor(1.0127, device='cuda:0')\n",
      "1 0.5369082714915223\n",
      "3 0.8419627494974354\n",
      "5 0.9481434477272757\n",
      "81 tensor(2.3268, device='cuda:0')\n",
      "1 0.542991283394292\n",
      "3 0.8504037621423464\n",
      "5 0.9512304968402568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [13:41:51<2:59:22, 597.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 tensor(0.9759, device='cuda:0')\n",
      "1 0.545694769968513\n",
      "3 0.8464556165517931\n",
      "5 0.9491881350303615\n",
      "82 tensor(2.3292, device='cuda:0')\n",
      "1 0.5458090593548683\n",
      "3 0.8503668991790128\n",
      "5 0.9523341717449947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [13:51:55<2:49:58, 599.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 tensor(0.9768, device='cuda:0')\n",
      "1 0.548853697448889\n",
      "3 0.8481121132055564\n",
      "5 0.9494718858597846\n",
      "83 tensor(2.3887, device='cuda:0')\n",
      "1 0.5384048211728022\n",
      "3 0.8480862191735841\n",
      "5 0.9520020249178328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 84/100 [14:01:57<2:40:07, 600.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 tensor(1.0234, device='cuda:0')\n",
      "1 0.5417634709823541\n",
      "3 0.8421734883613083\n",
      "5 0.9486588397112398\n",
      "84 tensor(2.4028, device='cuda:0')\n",
      "1 0.5391031323185239\n",
      "3 0.8512230771225543\n",
      "5 0.9501306515920763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [14:11:58<2:30:07, 600.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 tensor(1.0212, device='cuda:0')\n",
      "1 0.5419223809981568\n",
      "3 0.8448088406593275\n",
      "5 0.9478378294907354\n",
      "85 tensor(2.3895, device='cuda:0')\n",
      "1 0.5435493250653113\n",
      "3 0.8490018887117742\n",
      "5 0.9522727856578636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [14:21:59<2:20:09, 600.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 tensor(1.0040, device='cuda:0')\n",
      "1 0.5461995015249694\n",
      "3 0.843236923043388\n",
      "5 0.949569794006442\n",
      "86 tensor(2.4424, device='cuda:0')\n",
      "1 0.5372127281340182\n",
      "3 0.8453268305509432\n",
      "5 0.9494791693676995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 87/100 [14:32:01<2:10:14, 601.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 tensor(1.0570, device='cuda:0')\n",
      "1 0.5387015039387463\n",
      "3 0.8380058358482094\n",
      "5 0.946639989112318\n",
      "87 tensor(2.4032, device='cuda:0')\n",
      "1 0.5469940390712593\n",
      "3 0.8498261813771631\n",
      "5 0.9506606227519763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 88/100 [14:42:01<2:00:08, 600.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 tensor(1.0067, device='cuda:0')\n",
      "1 0.549116261100001\n",
      "3 0.84540283517169\n",
      "5 0.9483777121060091\n",
      "88 tensor(2.4026, device='cuda:0')\n",
      "1 0.5472351548150144\n",
      "3 0.8481717163616909\n",
      "5 0.950945620118884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 89/100 [14:51:59<1:49:59, 599.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 tensor(1.0036, device='cuda:0')\n",
      "1 0.5486423990310264\n",
      "3 0.8439834479729368\n",
      "5 0.9485551984839483\n",
      "89 tensor(2.4100, device='cuda:0')\n",
      "1 0.5482915167685709\n",
      "3 0.848395720343706\n",
      "5 0.9507278269874186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [15:02:03<1:40:12, 601.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 tensor(1.0013, device='cuda:0')\n",
      "1 0.5502476367643584\n",
      "3 0.8458868325454124\n",
      "5 0.9481948063076462\n",
      "90 tensor(2.4121, device='cuda:0')\n",
      "1 0.5474462761520095\n",
      "3 0.8493744415560656\n",
      "5 0.950427119465698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [15:12:03<1:30:06, 600.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 tensor(1.0010, device='cuda:0')\n",
      "1 0.5501252471507865\n",
      "3 0.8459562507474443\n",
      "5 0.9479568136864148\n",
      "91 tensor(2.4114, device='cuda:0')\n",
      "1 0.5495512755325198\n",
      "3 0.8515263154173973\n",
      "5 0.9512850551612152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [15:21:59<1:19:54, 599.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 tensor(1.0098, device='cuda:0')\n",
      "1 0.5505769445596633\n",
      "3 0.8461637607488719\n",
      "5 0.9492526497554754\n",
      "92 tensor(2.4531, device='cuda:0')\n",
      "1 0.5392924242757287\n",
      "3 0.8466223462341074\n",
      "5 0.9492304242418149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 93/100 [15:31:57<1:09:52, 598.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 tensor(1.0284, device='cuda:0')\n",
      "1 0.5429112947672134\n",
      "3 0.8433514645337001\n",
      "5 0.9469085209252722\n",
      "93 tensor(2.4357, device='cuda:0')\n",
      "1 0.5439961280405085\n",
      "3 0.8484465479926507\n",
      "5 0.9495975307566891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94/100 [15:41:59<59:59, 599.85s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 tensor(1.0148, device='cuda:0')\n",
      "1 0.5455697154152515\n",
      "3 0.8461003074929715\n",
      "5 0.9478659108607734\n",
      "94 tensor(2.4352, device='cuda:0')\n",
      "1 0.5480412107715795\n",
      "3 0.8493603041648103\n",
      "5 0.9503864434410275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95/100 [15:52:00<50:01, 600.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 tensor(1.0108, device='cuda:0')\n",
      "1 0.5497958091087441\n",
      "3 0.8466504426937441\n",
      "5 0.9485712930845158\n",
      "95 tensor(2.4282, device='cuda:0')\n",
      "1 0.5474631222286959\n",
      "3 0.8502781171361289\n",
      "5 0.9507039419646085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 96/100 [16:02:03<40:04, 601.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 tensor(1.0064, device='cuda:0')\n",
      "1 0.549626620861687\n",
      "3 0.8468032666735019\n",
      "5 0.9488294166549676\n",
      "96 tensor(2.4362, device='cuda:0')\n",
      "1 0.5466429302422986\n",
      "3 0.8521169023566549\n",
      "5 0.9512917185831767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 97/100 [16:11:49<29:49, 596.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 tensor(1.0098, device='cuda:0')\n",
      "1 0.5489671031861177\n",
      "3 0.848556116376153\n",
      "5 0.9491090971911539\n",
      "97 tensor(2.4399, device='cuda:0')\n",
      "1 0.548401322049544\n",
      "3 0.8503050998927814\n",
      "5 0.9512196278612922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98/100 [16:21:47<19:53, 596.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 tensor(1.0136, device='cuda:0')\n",
      "1 0.5507528628403177\n",
      "3 0.8471134020125959\n",
      "5 0.948892423852609\n",
      "98 tensor(2.4407, device='cuda:0')\n",
      "1 0.5466890203175988\n",
      "3 0.8510863200680615\n",
      "5 0.9506865601021182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 99/100 [16:31:41<09:56, 596.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 tensor(1.0175, device='cuda:0')\n",
      "1 0.5486819297363498\n",
      "3 0.8476043406935041\n",
      "5 0.9485449167326047\n",
      "99 tensor(2.4421, device='cuda:0')\n",
      "1 0.5479518428952148\n",
      "3 0.8502466512196935\n",
      "5 0.9499247653510022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [16:41:33<00:00, 600.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 tensor(1.0205, device='cuda:0')\n",
      "1 0.5501137483362526\n",
      "3 0.8473539130170674\n",
      "5 0.9477644358913689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "min_loss = -10000\n",
    "sampling_num = 3\n",
    "for epoch in tqdm(range(epoch)):\n",
    "    frame_encoder.train()\n",
    "    lang_encoder.train()\n",
    "    for iter_num in range(len(segment_dataset)):\n",
    "        with autocast():\n",
    "            btime_b = time.time()\n",
    "            batch = segment_dataset[iter_num]\n",
    "            #print(batch['encode'].shape,batch['trans_input_ids'].shape )\n",
    "            optim.zero_grad()\n",
    "            dotoutput, loss = retrieval_forward(batch, frame_encoder, lang_encoder,compute_loss = True)\n",
    "            writer.add_scalar('Loss/train',loss.detach().cpu(), epoch*len(segment_dataset)+iter_num)\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(list(frame_encoder.parameters()) + list(lang_encoder.parameters()),1 )\n",
    "        scaler.step(optim)\n",
    "        scaler.update()\n",
    "        torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        frame_encoder.eval()\n",
    "        lang_encoder.eval()\n",
    "        ks = [1,3,5]\n",
    "        recalls = {k:[] for k in ks}\n",
    "        losses = 0\n",
    "        for iter_num in range(len(valid_dataset)):\n",
    "            batch = valid_dataset[iter_num]\n",
    "            dotoutput, loss = retrieval_forward(batch, frame_encoder,  lang_encoder,compute_loss = True)\n",
    "            losses += loss\n",
    "            label = list(batch['label'].numpy())\n",
    "            for k in ks:\n",
    "                        pred = np.argsort(-1*dotoutput.detach().cpu().numpy(), axis = -1)[:,:k].squeeze()\n",
    "                        recallatk = 0\n",
    "                        examples = 0\n",
    "                        for i, gt in enumerate(label):\n",
    "                            if k > 1:\n",
    "                                if gt == -1:\n",
    "                                    continue\n",
    "                                if gt in pred[i]:\n",
    "                                    recallatk +=1\n",
    "                                examples += 1\n",
    "                            else:\n",
    "                                if gt == -1:\n",
    "                                    continue\n",
    "                                if gt == pred[i]:\n",
    "                                    recallatk +=1\n",
    "                                examples += 1\n",
    "                        recallatk = recallatk/examples\n",
    "                        recalls[k].append(recallatk)\n",
    "        losses = losses/len(valid_dataset)\n",
    "        writer.add_scalar('Loss/valid',losses.cpu(), epoch)\n",
    "        print(epoch, losses)\n",
    "        for k in ks:\n",
    "            print( k, sum(recalls[k])/len(recalls[k]))\n",
    "            writer.add_scalar(f'Recall/valid{k}',sum(recalls[k])/len(recalls[k]), epoch)\n",
    "        if min_loss<sum(recalls[1])/len(recalls[1]):\n",
    "                    min_loss = sum(recalls[1])/len(recalls[1])\n",
    "                    torch.save(lang_encoder.state_dict(), os.path.join(f'{mode}','lang_encoder_ori_best_eval_loss.pth'.format(epoch)))\n",
    "                    torch.save(frame_encoder.state_dict(), os.path.join(f'{mode}','image_encoder_ori_best_eval_loss.pth'.format(epoch)))\n",
    "        for iter_num in range(len(test_dataset)):\n",
    "            batch = test_dataset[iter_num]\n",
    "            dotoutput, loss = retrieval_forward(batch, frame_encoder, lang_encoder, compute_loss = True)\n",
    "            losses += loss\n",
    "            label = list(batch['label'].numpy())\n",
    "            for k in ks:\n",
    "                        pred = np.argsort(-1*dotoutput.detach().cpu().numpy(), axis = -1)[:,:k].squeeze()\n",
    "                        recallatk = 0\n",
    "                        examples = 0\n",
    "                        for i, gt in enumerate(label):\n",
    "                            if k > 1:\n",
    "                                if gt == -1:\n",
    "                                    continue\n",
    "                                if gt in pred[i]:\n",
    "                                    recallatk +=1\n",
    "                                examples += 1\n",
    "                            else:\n",
    "                                if gt == -1:\n",
    "                                    continue\n",
    "                                if gt == pred[i]:\n",
    "                                    recallatk +=1\n",
    "                                examples += 1\n",
    "                        recallatk = recallatk/examples\n",
    "                        recalls[k].append(recallatk)\n",
    "        losses = losses/len(valid_dataset)\n",
    "        writer.add_scalar('Loss/test',losses.cpu(), epoch)\n",
    "        print(epoch, losses)\n",
    "        for k in ks:\n",
    "                print( k, sum(recalls[k])/len(recalls[k]))\n",
    "                writer.add_scalar(f'Recall/test{k}',sum(recalls[k])/len(recalls[k]), epoch)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vidpkl = pkl.load(open(os.path.join('COOK2_IVD','pkl','vid.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_encoder_state_dict = torch.load(os.path.join(mode, 'image_encoder_ori_best_eval_loss.pth'))\n",
    "frame_encoder.load_state_dict(frame_encoder_state_dict)\n",
    "lang_encoder_state_dict = torch.load(os.path.join(mode, 'lang_encoder_ori_best_eval_loss.pth'))\n",
    "lang_encoder.load_state_dict(lang_encoder_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 tensor(2.7031, device='cuda:0')\n",
      "1 0.5551631858077444\n",
      "3 0.8598434427384077\n",
      "5 0.9522972897459667\n",
      "99 tensor(1.1041, device='cuda:0')\n",
      "1 0.5587508731452381\n",
      "3 0.8555428180545263\n",
      "5 0.9489275610512035\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "        frame_encoder.eval()\n",
    "        lang_encoder.eval()\n",
    "        ks = [1,3,5]\n",
    "        recalls = {k:[] for k in ks}\n",
    "        losses = 0\n",
    "        for iter_num in range(len(valid_dataset)):\n",
    "            batch = valid_dataset[iter_num]\n",
    "            dotoutput, loss = retrieval_forward(batch, frame_encoder, lang_encoder,epoch, compute_loss = True)\n",
    "            losses += loss\n",
    "            label = list(batch['label'].numpy())\n",
    "            for k in ks:\n",
    "                        pred = np.argsort(-1*dotoutput.detach().cpu().numpy(), axis = -1)[:,:k].squeeze()\n",
    "                        recallatk = 0\n",
    "                        examples = 0\n",
    "                        for i, gt in enumerate(label):\n",
    "                            if k > 1:\n",
    "                                if gt == -1:\n",
    "                                    continue\n",
    "                                if gt in pred[i]:\n",
    "                                    recallatk +=1\n",
    "                                examples += 1\n",
    "                            else:\n",
    "                                if gt == -1:\n",
    "                                    continue\n",
    "                                if gt == pred[i]:\n",
    "                                    recallatk +=1\n",
    "                                examples += 1\n",
    "                        recallatk = recallatk/examples\n",
    "                        recalls[k].append(recallatk)\n",
    "        losses = losses/len(valid_dataset)\n",
    "        writer.add_scalar('Loss/valid',losses.cpu(), epoch)\n",
    "        print(epoch, losses)\n",
    "        for k in ks:\n",
    "            print( k, sum(recalls[k])/len(recalls[k]))\n",
    "        for iter_num in range(len(test_dataset)):\n",
    "            batch = test_dataset[iter_num]\n",
    "            dotoutput, loss = retrieval_forward(batch, frame_encoder, lang_encoder,epoch, compute_loss = True)\n",
    "            losses += loss\n",
    "            label = list(batch['label'].numpy())\n",
    "            for k in ks:\n",
    "                        pred = np.argsort(-1*dotoutput.detach().cpu().numpy(), axis = -1)[:,:k].squeeze()\n",
    "                        recallatk = 0\n",
    "                        examples = 0\n",
    "                        for i, gt in enumerate(label):\n",
    "                            if k > 1:\n",
    "                                if gt == -1:\n",
    "                                    continue\n",
    "                                if gt in pred[i]:\n",
    "                                    recallatk +=1\n",
    "                                examples += 1\n",
    "                            else:\n",
    "                                if gt == -1:\n",
    "                                    continue\n",
    "                                if gt == pred[i]:\n",
    "                                    recallatk +=1\n",
    "                                examples += 1\n",
    "                        recallatk = recallatk/examples\n",
    "                        recalls[k].append(recallatk)\n",
    "        losses = losses/len(valid_dataset)\n",
    "        print(epoch, losses)\n",
    "        for k in ks:\n",
    "                print( k, sum(recalls[k])/len(recalls[k]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:37<00:00,  3.64it/s]\n",
      "  0%|          | 0/312 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tensor(1.0021, device='cuda:0')\n",
      "1 0.552947379644617\n",
      "3 0.8337703010705011\n",
      "5 0.9445555350399873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [01:19<00:00,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tensor(2.4114, device='cuda:0')\n",
      "1 0.5495512755325198\n",
      "3 0.8515263154173973\n",
      "5 0.9512850551612152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "retrieval_pred = {}\n",
    "with torch.no_grad():\n",
    "        frame_encoder.eval()\n",
    "        lang_encoder.eval()\n",
    "        ks = [1,3,5]\n",
    "        recalls = {k:[] for k in ks}\n",
    "        losses = 0\n",
    "        for iter_num in tqdm(range(len(test_dataset))):\n",
    "            vid_pred = {}\n",
    "            batch = test_dataset[iter_num]\n",
    "            dotoutput, loss = retrieval_forward(batch, frame_encoder, lang_encoder, compute_loss = True)\n",
    "            losses += loss\n",
    "            label = list(batch['label'].numpy())\n",
    "            for k in ks:\n",
    "                        pred = np.argsort(-1*dotoutput.detach().cpu().numpy(), axis = -1)[:,:k].squeeze()\n",
    "                        recallatk = 0\n",
    "                        examples = 0\n",
    "                        for i, gt in enumerate(label):\n",
    "                            if k > 1:\n",
    "                                if gt == -1:\n",
    "                                    continue\n",
    "                                if gt in pred[i]:\n",
    "                                    recallatk +=1\n",
    "                                examples += 1\n",
    "                            else:\n",
    "                                vid_pred[2*i+1] = pred[i]\n",
    "                                if gt == -1:\n",
    "                                    continue\n",
    "                                if gt == pred[i]:\n",
    "                                    recallatk +=1\n",
    "                                examples += 1\n",
    "                        recallatk = recallatk/examples\n",
    "                        recalls[k].append(recallatk)\n",
    "            retrieval_pred[batch['vid']] = vid_pred\n",
    "        losses = losses/len(valid_dataset)\n",
    "        print(epoch, losses)\n",
    "        for k in ks:\n",
    "                print( k, sum(recalls[k])/len(recalls[k]))\n",
    "with torch.no_grad():\n",
    "        frame_encoder.eval()\n",
    "        lang_encoder.eval()\n",
    "        ks = [1,3,5]\n",
    "        recalls = {k:[] for k in ks}\n",
    "        losses = 0\n",
    "        for iter_num in tqdm(range(len(valid_dataset))):\n",
    "            vid_pred = {}\n",
    "            batch = valid_dataset[iter_num]\n",
    "            dotoutput, loss = retrieval_forward(batch, frame_encoder, lang_encoder, compute_loss = True)\n",
    "            losses += loss\n",
    "            label = list(batch['label'].numpy())\n",
    "            for k in ks:\n",
    "                        pred = np.argsort(-1*dotoutput.detach().cpu().numpy(), axis = -1)[:,:k].squeeze()\n",
    "                        recallatk = 0\n",
    "                        examples = 0\n",
    "                        for i, gt in enumerate(label):\n",
    "                            if k > 1:\n",
    "                                if gt == -1:\n",
    "                                    continue\n",
    "                                if gt in pred[i]:\n",
    "                                    recallatk +=1\n",
    "                                examples += 1\n",
    "                            else:\n",
    "                                vid_pred[2*i+1] = pred[i]\n",
    "                                if gt == -1:\n",
    "                                    continue\n",
    "                                if gt == pred[i]:\n",
    "                                    recallatk +=1\n",
    "                                examples += 1\n",
    "                        recallatk = recallatk/examples\n",
    "                        recalls[k].append(recallatk)\n",
    "            retrieval_pred[batch['vid']] = vid_pred\n",
    "        losses = losses/len(valid_dataset)\n",
    "        print(epoch, losses)\n",
    "        for k in ks:\n",
    "                print( k, sum(recalls[k])/len(recalls[k]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_threshold=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:38<00:00,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tensor(1.0021, device='cuda:0')\n",
      "1 0.5516363098202698\n",
      "3 0.8204740007479437\n",
      "5 0.9387999926629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "retrieval_pred = {}\n",
    "retrieval_label = {}\n",
    "with torch.no_grad():\n",
    "        frame_encoder.eval()\n",
    "        lang_encoder.eval()\n",
    "        ks = [1,3,5]\n",
    "        recalls = {k:[] for k in ks}\n",
    "        losses = 0\n",
    "        for iter_num in tqdm(range(len(test_dataset))):\n",
    "            vid_pred = {}\n",
    "            vid_label = {}\n",
    "            batch = test_dataset[iter_num]\n",
    "            vid = batch['vid']\n",
    "            vid_iou = ious[vid]\n",
    "            dotoutput, loss = retrieval_forward(batch, frame_encoder, lang_encoder, compute_loss = True)\n",
    "            losses += loss\n",
    "            label = list(batch['label'].numpy())\n",
    "            vid_labels = list(set(label))\n",
    "            vid_labels.remove(-1)\n",
    "            hard_segs = []\n",
    "            for i in vid_labels:\n",
    "                if i==0:\n",
    "                    if vid_iou[(0,1)]<iou_threshold:\n",
    "                        hard_segs.append(i)\n",
    "                elif i == max(vid_labels):\n",
    "                    if vid_iou[(i-1,i)]<iou_threshold:\n",
    "                        hard_segs.append(i)\n",
    "                else:\n",
    "                    if vid_iou[(i-1,i)]<iou_threshold and vid_iou[(i,i+1)]<iou_threshold:\n",
    "                        hard_segs.append(i)\n",
    "            for k in ks:\n",
    "                        pred = np.argsort(-1*dotoutput.detach().cpu().numpy(), axis = -1)[:,:k].squeeze()\n",
    "                        recallatk = 0\n",
    "                        examples = 0\n",
    "                        for i, gt in enumerate(label):\n",
    "                            if gt not in hard_segs:\n",
    "                                continue\n",
    "                            if k > 1:\n",
    "                                if gt == -1:\n",
    "                                    continue\n",
    "                                if gt in pred[i]:\n",
    "                                    recallatk +=1\n",
    "                                examples += 1\n",
    "                            else:\n",
    "                                vid_pred[2*i+1] = pred[i]\n",
    "                                vid_label[2*i+1] = gt\n",
    "                                if gt == -1:\n",
    "                                    continue\n",
    "                                if gt == pred[i]:\n",
    "                                    recallatk +=1\n",
    "                                examples += 1\n",
    "                        if examples<1:\n",
    "                            continue\n",
    "                        recallatk = recallatk/examples\n",
    "                        recalls[k].append(recallatk)\n",
    "            retrieval_pred[batch['vid']] = vid_pred\n",
    "            retrieval_label[batch['vid']] = vid_label\n",
    "        losses = losses/len(valid_dataset)\n",
    "        print(epoch, losses)\n",
    "        for k in ks:\n",
    "                print( k, sum(recalls[k])/len(recalls[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(retrieval_pred, open('procedure_contrast_retrieval_result.json', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "99 tensor(2.5399, device='cuda:0')\n",
    "1 0.5500848235384761\n",
    "3 0.8597046990109899\n",
    "5 0.9546420315049812\n",
    "99 tensor(1.0750, device='cuda:0')\n",
    "1 0.5493910851437317\n",
    "3 0.8543261536011475\n",
    "5 0.9503941514540208\n",
    "class sequence_frame_encoder(torch.nn.Module):\n",
    "    def __init__(self, config, sequence_encoder = 'LSTM'):\n",
    "        super(sequence_frame_encoder, self).__init__()\n",
    "        self.frame_encoder = LXRTFeatureExtraction(config)\n",
    "        state_dict_path = os.path.join('lxmert', 'snap', 'pretrained', 'model_LXRT.pth') \n",
    "        state_dict = torch.load(state_dict_path)\n",
    "        new_state_dict = OrderedDict()\n",
    "        for key, value in state_dict.items():\n",
    "            splittedkey = key.split('.')\n",
    "            if 'bert' in splittedkey:\n",
    "                newkey  = '.'.join(splittedkey[splittedkey.index('bert')+1:])\n",
    "            else:\n",
    "                newkey  = '.'.join(splittedkey[splittedkey.index('module')+1:])\n",
    "            new_state_dict[newkey] = value\n",
    "        self.frame_encoder.load_state_dict(new_state_dict, strict=False)\n",
    "        del state_dict\n",
    "        del new_state_dict\n",
    "        if sequence_encoder == 'LSTM':\n",
    "            self.sequence_encoder = torch.nn.LSTM(768, hidden_size = 768//2, batch_first = True, bidirectional = True)\n",
    "            self.sequence_fc = torch.nn.Linear(768, 768)\n",
    "            self.contrast_fc = torch.nn.Linear(768,768)\n",
    "            \n",
    "        #transformer later\n",
    "        #else:\n",
    "        #    self.sequence_encoder = torch.nn.TransformerEncoderLayer(d_model = 768,nhead = 12,num_encoder_layers = 1,\n",
    "        #                                                             dim_feedforward=3072, activation == \"gelu\", batch_first = True )\n",
    "    def forward(self,visn_feats, trans_input_ids,trans_token_type_ids,trans_attention_mask ):\n",
    "        frame_feats = self.frame_encoder(trans_input_ids, \n",
    "                                        token_type_ids = trans_token_type_ids,\n",
    "                                        attention_mask = trans_attention_mask,\n",
    "                                        visual_feats = visn_feats\n",
    "                                       )\n",
    "        frame_feats = frame_feats[1]        \n",
    "        #frame_contrast = self.contrast_fc(frame_feats)\n",
    "        #frame_contrast = frame_contrast.squeeze()\n",
    "        frame_feats = frame_feats.unsqueeze(0)\n",
    "        frame_feats,(_,_) = self.sequence_encoder(frame_feats)\n",
    "        frame_contrast = self.contrast_fc(frame_feats)\n",
    "        frame_contrast = frame_contrast.squeeze()\n",
    "        frame_feats = self.sequence_fc(frame_feats)\n",
    "        frame_feats = frame_feats.squeeze()\n",
    "        return frame_feats, frame_contrast\n",
    "99 tensor(2.3990, device='cuda:0')\n",
    "1 0.5509897188919297\n",
    "3 0.8555678469627827\n",
    "5 0.9494686560239055\n",
    "99 tensor(1.0209, device='cuda:0')\n",
    "1 0.5510184906637344\n",
    "3 0.8487851879782635\n",
    "5 0.9473827653403941\n",
    "class sequence_frame_encoder(torch.nn.Module):\n",
    "    def __init__(self, config, sequence_encoder = 'LSTM'):\n",
    "        super(sequence_frame_encoder, self).__init__()\n",
    "        self.frame_encoder = LXRTFeatureExtraction(config)\n",
    "        state_dict_path = os.path.join('lxmert', 'snap', 'pretrained', 'model_LXRT.pth') \n",
    "        state_dict = torch.load(state_dict_path)\n",
    "        new_state_dict = OrderedDict()\n",
    "        for key, value in state_dict.items():\n",
    "            splittedkey = key.split('.')\n",
    "            if 'bert' in splittedkey:\n",
    "                newkey  = '.'.join(splittedkey[splittedkey.index('bert')+1:])\n",
    "            else:\n",
    "                newkey  = '.'.join(splittedkey[splittedkey.index('module')+1:])\n",
    "            new_state_dict[newkey] = value\n",
    "        self.frame_encoder.load_state_dict(new_state_dict, strict=False)\n",
    "        del state_dict\n",
    "        del new_state_dict\n",
    "        if sequence_encoder == 'LSTM':\n",
    "            self.sequence_encoder = torch.nn.LSTM(768, hidden_size = 768//2, batch_first = True, bidirectional = True)\n",
    "            self.sequence_fc = torch.nn.Linear(768, 768)\n",
    "            self.contrast_fc = torch.nn.Linear(768,768)\n",
    "            \n",
    "        #transformer later\n",
    "        #else:\n",
    "        #    self.sequence_encoder = torch.nn.TransformerEncoderLayer(d_model = 768,nhead = 12,num_encoder_layers = 1,\n",
    "        #                                                             dim_feedforward=3072, activation == \"gelu\", batch_first = True )\n",
    "    def forward(self,visn_feats, trans_input_ids,trans_token_type_ids,trans_attention_mask ):\n",
    "        frame_feats = self.frame_encoder(trans_input_ids, \n",
    "                                        token_type_ids = trans_token_type_ids,\n",
    "                                        attention_mask = trans_attention_mask,\n",
    "                                        visual_feats = visn_feats\n",
    "                                       )\n",
    "        frame_feats = frame_feats[1]        \n",
    "        frame_contrast = self.contrast_fc(frame_feats)\n",
    "        frame_contrast = frame_contrast.squeeze()\n",
    "        frame_feats = frame_feats.unsqueeze(0)\n",
    "        frame_feats,(_,_) = self.sequence_encoder(frame_feats)\n",
    "        #frame_contrast = self.contrast_fc(frame_feats)\n",
    "        #frame_contrast = frame_contrast.squeeze()\n",
    "        frame_feats = self.sequence_fc(frame_feats)\n",
    "        frame_feats = frame_feats.squeeze()\n",
    "        return frame_feats, frame_contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('yc2_recipes.json', 'r') as fp:\n",
    "    recipes = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vid2frame_indices.pkl', 'rb') as fp:\n",
    "    frame_indices = pkl.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#encode = encode.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "recipe_retrieval_result = {}\n",
    "test_retrieval_result_all = {}\n",
    "with torch.no_grad():\n",
    "    with autocast(): \n",
    "        for iter_num in range(len(segment_dataset)):\n",
    "            batch = segment_dataset[iter_num]\n",
    "            vid, visn_feats, encode, label = batch['vid'], batch[\"visn_feats\"],batch['encode'],batch['label']\n",
    "            frame_feats, box_feats = visn_feats\n",
    "            foodname = batch['query']\n",
    "            vid_frame_indices = frame_indices[vid]\n",
    "            trans_input_ids,trans_token_type_ids,trans_attention_mask  = batch['trans_input_ids'],batch['trans_token_type_ids'],batch['trans_attention_mask']\n",
    "            label = label.cuda()\n",
    "            visn_feats = frame_feats.cuda(), box_feats.cuda()\n",
    "            trans_input_ids = trans_input_ids.cuda()\n",
    "            trans_token_type_ids = trans_token_type_ids.cuda()\n",
    "            trans_attention_mask = trans_attention_mask.cuda()\n",
    "            vid_recipe = recipes[foodname]\n",
    "            #recipe_logit[vid] = {}\n",
    "            flatten_recipe = []\n",
    "            for recipe in vid_recipe:\n",
    "                for recipe_doc in recipe['split_ins']:\n",
    "                    if recipe_doc!= '':\n",
    "                        flatten_recipe.append(recipe_doc)\n",
    "            encode = tokenizer.batch_encode_plus(flatten_recipe, return_tensors = 'pt',padding = True)\n",
    "            frame_feats,_ = frame_encoder(visn_feats, trans_input_ids, \n",
    "                                                    trans_token_type_ids,trans_attention_mask\n",
    "                                                   )\n",
    "            pooled_output = frame_feats\n",
    "            output = lang_encoder(input_ids = encode['input_ids'].cuda(), token_type_ids = encode['token_type_ids'].cuda(),\n",
    "                                  attention_mask = encode['attention_mask'].cuda())\n",
    "            sequence_output, lang_pooled_output = output[0], output[1]\n",
    "            dotoutput = torch.matmul(pooled_output, lang_pooled_output.transpose(1,0))\n",
    "            retrieval_result = torch.topk(dotoutput,k=3, dim = -1)[1].tolist()\n",
    "            vid_retrieval_result = {vid_frame_indices[i]:[flatten_recipe[j] for j in retrieval_result[i]] for i in range(len(vid_frame_indices))}\n",
    "            recipe_retrieval_result[vid] = vid_retrieval_result\n",
    "        for iter_num in range(len(valid_dataset)):\n",
    "            batch = valid_dataset[iter_num]\n",
    "            vid, visn_feats, encode, label = batch['vid'], batch[\"visn_feats\"],batch['encode'],batch['label']\n",
    "            frame_feats, box_feats = visn_feats\n",
    "            foodname = batch['query']\n",
    "            vid_frame_indices = frame_indices[vid]\n",
    "            trans_input_ids,trans_token_type_ids,trans_attention_mask  = batch['trans_input_ids'],batch['trans_token_type_ids'],batch['trans_attention_mask']\n",
    "            label = label.cuda()\n",
    "            visn_feats = frame_feats.cuda(), box_feats.cuda()\n",
    "            trans_input_ids = trans_input_ids.cuda()\n",
    "            trans_token_type_ids = trans_token_type_ids.cuda()\n",
    "            trans_attention_mask = trans_attention_mask.cuda()\n",
    "            vid_recipe = recipes[foodname]\n",
    "            #recipe_logit[vid] = {}\n",
    "            flatten_recipe = []\n",
    "            for recipe in vid_recipe:\n",
    "                for recipe_doc in recipe['split_ins']:\n",
    "                    if recipe_doc!= '':\n",
    "                        flatten_recipe.append(recipe_doc)\n",
    "            encode = tokenizer.batch_encode_plus(flatten_recipe, return_tensors = 'pt',padding = True)\n",
    "            frame_feats,_ = frame_encoder(visn_feats, trans_input_ids, \n",
    "                                                    trans_token_type_ids,trans_attention_mask\n",
    "                                                   )\n",
    "            pooled_output = frame_feats\n",
    "            output = lang_encoder(input_ids = encode['input_ids'].cuda(), token_type_ids = encode['token_type_ids'].cuda(),\n",
    "                                  attention_mask = encode['attention_mask'].cuda())\n",
    "            sequence_output, lang_pooled_output = output[0], output[1]\n",
    "            dotoutput = torch.matmul(pooled_output, lang_pooled_output.transpose(1,0))\n",
    "            retrieval_result = torch.topk(dotoutput,k=3, dim = -1)[1].tolist()\n",
    "            vid_retrieval_result = {vid_frame_indices[i]:[flatten_recipe[j] for j in retrieval_result[i]] for i in range(len(vid_frame_indices))}\n",
    "            recipe_retrieval_result[vid] = vid_retrieval_result\n",
    "        for iter_num in range(len(test_dataset)):\n",
    "            batch = test_dataset[iter_num]\n",
    "            vid, visn_feats, encode, label = batch['vid'], batch[\"visn_feats\"],batch['encode'],batch['label']\n",
    "            frame_feats, box_feats = visn_feats\n",
    "            foodname = batch['query']\n",
    "            vid_frame_indices = frame_indices[vid]\n",
    "            trans_input_ids,trans_token_type_ids,trans_attention_mask  = batch['trans_input_ids'],batch['trans_token_type_ids'],batch['trans_attention_mask']\n",
    "            label = label.cuda()\n",
    "            visn_feats = frame_feats.cuda(), box_feats.cuda()\n",
    "            trans_input_ids = trans_input_ids.cuda()\n",
    "            trans_token_type_ids = trans_token_type_ids.cuda()\n",
    "            trans_attention_mask = trans_attention_mask.cuda()\n",
    "            vid_recipe = recipes[foodname]\n",
    "            #recipe_logit[vid] = {}\n",
    "            flatten_recipe = []\n",
    "            for recipe in vid_recipe:\n",
    "                for recipe_doc in recipe['split_ins']:\n",
    "                    if recipe_doc!= '':\n",
    "                        flatten_recipe.append(recipe_doc)\n",
    "            encode = tokenizer.batch_encode_plus(flatten_recipe, return_tensors = 'pt',padding = True)\n",
    "            frame_feats,_ = frame_encoder(visn_feats, trans_input_ids, \n",
    "                                                    trans_token_type_ids,trans_attention_mask\n",
    "                                                   )\n",
    "            pooled_output = frame_feats\n",
    "            output = lang_encoder(input_ids = encode['input_ids'].cuda(), token_type_ids = encode['token_type_ids'].cuda(),\n",
    "                                  attention_mask = encode['attention_mask'].cuda())\n",
    "            sequence_output, lang_pooled_output = output[0], output[1]\n",
    "            dotoutput = torch.matmul(pooled_output, lang_pooled_output.transpose(1,0))\n",
    "            retrieval_result = torch.topk(dotoutput,k=3, dim = -1)[1].tolist()\n",
    "            vid_retrieval_result = {vid_frame_indices[i]:[flatten_recipe[j] for j in retrieval_result[i]] for i in range(len(vid_frame_indices))}\n",
    "            recipe_retrieval_result[vid] = vid_retrieval_result\n",
    "            retrieval_result_all = np.argsort(-1*dotoutput.detach().cpu().numpy())\n",
    "            vid_retrieval_result_all = {vid_frame_indices[i]:[flatten_recipe[j] for j in retrieval_result_all[i]] for i in range(len(vid_frame_indices))}\n",
    "            test_retrieval_result_all[vid] = vid_retrieval_result_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(test_retrieval_result_all, open('lxmert_sequence_procedure_contrast_recipe_test_retrieval_result_all.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[178, 59, 79],\n",
       " [2, 79, 82],\n",
       " [2, 79, 82],\n",
       " [2, 60, 79],\n",
       " [2, 60, 0],\n",
       " [2, 60, 82],\n",
       " [60, 2, 27],\n",
       " [0, 27, 177],\n",
       " [0, 174, 60],\n",
       " [0, 174, 60],\n",
       " [60, 0, 174],\n",
       " [60, 0, 179],\n",
       " [12, 60, 179],\n",
       " [179, 12, 60],\n",
       " [179, 12, 108],\n",
       " [108, 12, 166],\n",
       " [179, 108, 12],\n",
       " [67, 108, 46],\n",
       " [67, 46, 81],\n",
       " [67, 104, 7],\n",
       " [104, 179, 68],\n",
       " [179, 104, 68],\n",
       " [0, 179, 174],\n",
       " [174, 0, 12],\n",
       " [179, 104, 174],\n",
       " [179, 104, 81],\n",
       " [12, 67, 60],\n",
       " [12, 67, 60],\n",
       " [12, 38, 179],\n",
       " [179, 38, 12],\n",
       " [60, 38, 12],\n",
       " [60, 38, 179],\n",
       " [60, 179, 165],\n",
       " [179, 38, 104],\n",
       " [38, 12, 81],\n",
       " [67, 12, 38],\n",
       " [81, 46, 116],\n",
       " [81, 46, 42],\n",
       " [116, 46, 131],\n",
       " [46, 116, 131],\n",
       " [46, 131, 42],\n",
       " [46, 81, 131],\n",
       " [81, 46, 144],\n",
       " [81, 144, 6],\n",
       " [144, 97, 87],\n",
       " [86, 169, 160],\n",
       " [9, 47, 57],\n",
       " [9, 47, 77],\n",
       " [47, 9, 87],\n",
       " [87, 29, 47],\n",
       " [29, 87, 53],\n",
       " [29, 99, 53],\n",
       " [18, 120, 53],\n",
       " [109, 184, 48],\n",
       " [48, 184, 109],\n",
       " [184, 48, 120],\n",
       " [184, 48, 109],\n",
       " [184, 48, 120],\n",
       " [184, 120, 109],\n",
       " [30, 135, 184],\n",
       " [135, 66, 30],\n",
       " [135, 66, 38],\n",
       " [135, 38, 66],\n",
       " [135, 66, 38]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[178,  59,  79, ..., 107, 109, 172],\n",
       "       [  2,  79,  82, ...,  85,  53, 109],\n",
       "       [  2,  79, 177, ...,  85,  53, 109],\n",
       "       ...,\n",
       "       [135,  66,  38, ..., 130, 156, 164],\n",
       "       [135,  38,  66, ..., 130, 156, 164],\n",
       "       [135,  66,  38, ..., 164,  85, 156]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(-1*dotoutput.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.7   ,   3.46  ,  10.68  , ...,  -0.415 ,   0.2246,  -4.71  ],\n",
       "       [ 13.164 ,   5.08  ,  16.03  , ...,  -0.7393,   0.4844,  -7.074 ],\n",
       "       [ 17.67  ,   4.477 ,  21.44  , ...,  -1.004 ,  -2.32  , -11.25  ],\n",
       "       ...,\n",
       "       [  0.4785,   3.332 ,  -0.2866, ...,   0.0984,   1.104 ,   5.062 ],\n",
       "       [  3.555 ,   3.045 ,   3.95  , ...,   0.1044,   0.124 ,   2.402 ],\n",
       "       [  4.914 ,   0.5605,   6.5   , ...,   0.7183,  -1.6045,   1.834 ]],\n",
       "      dtype=float16)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotoutput.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(recipe_retrieval_result, open('lxmert_sequence_procedure_contrast_recipe_retrieval_result.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(recipe_retrieval_result, open('lxmert_sequence_procedure_contrast_recipe_retrieval_result.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method forward in module transformers.models.bert.modeling_bert:\n",
      "\n",
      "forward(input_ids: Union[torch.Tensor, NoneType] = None, attention_mask: Union[torch.Tensor, NoneType] = None, token_type_ids: Union[torch.Tensor, NoneType] = None, position_ids: Union[torch.Tensor, NoneType] = None, head_mask: Union[torch.Tensor, NoneType] = None, inputs_embeds: Union[torch.Tensor, NoneType] = None, encoder_hidden_states: Union[torch.Tensor, NoneType] = None, encoder_attention_mask: Union[torch.Tensor, NoneType] = None, past_key_values: Union[List[torch.FloatTensor], NoneType] = None, use_cache: Union[bool, NoneType] = None, output_attentions: Union[bool, NoneType] = None, output_hidden_states: Union[bool, NoneType] = None, return_dict: Union[bool, NoneType] = None) -> Union[Tuple[torch.Tensor], transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions] method of transformers.models.bert.modeling_bert.BertModel instance\n",
      "    The [`BertModel`] forward method, overrides the `__call__` special method.\n",
      "    \n",
      "    <Tip>\n",
      "    \n",
      "    Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
      "    instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
      "    the latter silently ignores them.\n",
      "    \n",
      "    </Tip>\n",
      "    \n",
      "    Args:\n",
      "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "            Indices of input sequence tokens in the vocabulary.\n",
      "    \n",
      "            Indices can be obtained using [`BertTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
      "            [`PreTrainedTokenizer.__call__`] for details.\n",
      "    \n",
      "            [What are input IDs?](../glossary#input-ids)\n",
      "        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
      "    \n",
      "            - 1 for tokens that are **not masked**,\n",
      "            - 0 for tokens that are **masked**.\n",
      "    \n",
      "            [What are attention masks?](../glossary#attention-mask)\n",
      "        token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n",
      "            1]`:\n",
      "    \n",
      "            - 0 corresponds to a *sentence A* token,\n",
      "            - 1 corresponds to a *sentence B* token.\n",
      "    \n",
      "            [What are token type IDs?](../glossary#token-type-ids)\n",
      "        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
      "            config.max_position_embeddings - 1]`.\n",
      "    \n",
      "            [What are position IDs?](../glossary#position-ids)\n",
      "        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n",
      "            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n",
      "    \n",
      "            - 1 indicates the head is **not masked**,\n",
      "            - 0 indicates the head is **masked**.\n",
      "    \n",
      "        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
      "            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
      "            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n",
      "            model's internal embedding lookup matrix.\n",
      "        output_attentions (`bool`, *optional*):\n",
      "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
      "            tensors for more detail.\n",
      "        output_hidden_states (`bool`, *optional*):\n",
      "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
      "            more detail.\n",
      "        return_dict (`bool`, *optional*):\n",
      "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      "    \n",
      "        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
      "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
      "            the model is configured as a decoder.\n",
      "        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
      "            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n",
      "    \n",
      "            - 1 for tokens that are **not masked**,\n",
      "            - 0 for tokens that are **masked**.\n",
      "        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
      "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
      "    \n",
      "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
      "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
      "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
      "        use_cache (`bool`, *optional*):\n",
      "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
      "            `past_key_values`).\n",
      "        \n",
      "    Returns:\n",
      "        [`transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions`] or `tuple(torch.FloatTensor)`: A [`transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions`] or a tuple of\n",
      "        `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
      "        elements depending on the configuration ([`BertConfig`]) and inputs.\n",
      "    \n",
      "        - **last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-states at the output of the last layer of the model.\n",
      "        - **pooler_output** (`torch.FloatTensor` of shape `(batch_size, hidden_size)`) -- Last layer hidden-state of the first token of the sequence (classification token) after further processing\n",
      "          through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns\n",
      "          the classification token after processing through a linear layer and a tanh activation function. The linear\n",
      "          layer weights are trained from the next sentence prediction (classification) objective during pretraining.\n",
      "        - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
      "          one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
      "    \n",
      "          Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
      "        - **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
      "          sequence_length)`.\n",
      "    \n",
      "          Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
      "          heads.\n",
      "        - **cross_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
      "          sequence_length)`.\n",
      "    \n",
      "          Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n",
      "          weighted average in the cross-attention heads.\n",
      "        - **past_key_values** (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`) -- Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "          `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n",
      "          `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n",
      "          encoder_sequence_length, embed_size_per_head)`.\n",
      "    \n",
      "          Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n",
      "          `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n",
      "          input) to speed up sequential decoding.\n",
      "    \n",
      "    Example:\n",
      "    \n",
      "    ```python\n",
      "    >>> from transformers import BertTokenizer, BertModel\n",
      "    >>> import torch\n",
      "    \n",
      "    >>> tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
      "    >>> model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
      "    \n",
      "    >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
      "    >>> outputs = model(**inputs)\n",
      "    \n",
      "    >>> last_hidden_states = outputs.last_hidden_state\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dotoutput), len(vid_frame_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = batch['vid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(43.0, 55.0, 'add the cabbage and water to a pan'),\n",
       " (62.0, 76.0, 'add cream and butter to a pot'),\n",
       " (112.0, 120.0, 'add the cabbage to the pot of potatos'),\n",
       " (120.0, 127.0, 'add the green onions to the pot'),\n",
       " (127.0, 143.0, 'add the bacon and pepper to the pot'),\n",
       " (143.0, 151.0, 'mash the ingredients in the pot')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vidpkl[vid]['segments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
